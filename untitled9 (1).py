# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EuX2428bBfjlaXmc2FLJA3hlHs4zUM38
"""

# Cell 1: Install dependencies
# Install all dependencies (no fixed Whisper version)
!pip install -q openai-whisper torch torchaudio reportlab jiwer python-dotenv rapidfuzz openai
#!pip install -q openai-whisper==20230314.1   # whisper
!pip install -q torch torchaudio               # torch & torchaudio
!pip install -q reportlab jiwer python-dotenv rapidfuzz
!pip install -q openai                         # OpenAI client for optional LLM

# Cell 2: Imports and configuration
import os
import json
import re
import uuid
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# Set your OpenAI key here if you want to use OpenAI for summarization.
# If you do not want to use an LLM or do not have a key, set this to an empty string.
OPENAI_API_KEY = ""  # <-- Replace with your OpenAI API key or leave empty to use fallback summarizer
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# Import packages
import whisper
import openai
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from jiwer import wer
from rapidfuzz import fuzz

# Configure OpenAI client if key provided
if OPENAI_API_KEY:
    openai.api_key = YOUR_API_KEY

# Cell 3: MeetingSummarizer class
class MeetingSummarizer:
    def __init__(self, whisper_model: str = "base", use_gpu: bool = False):
        """
        whisper_model: one of 'tiny', 'base', 'small', 'medium', 'large'
        use_gpu: set True if Colab runtime has GPU enabled and you want to use it
        """
        self.device = "cuda" if use_gpu else "cpu"
        # load model (whisper will handle CPU / GPU automatically according to device param)
        self.model = whisper.load_model(whisper_model, device=self.device)
        # LLM configuration
        self.llm_available = bool(OPENAI_API_KEY)
        self.results = {}

    def transcribe_audio(self, audio_path: str, language: str = None) -> Dict[str, Any]:
        """
        Transcribe audio using Whisper.
        Returns a dict with transcript, segments, language, timestamp.
        """
        if not Path(audio_path).exists():
            raise FileNotFoundError(f"Audio file not found: {audio_path}")

        # Use whisper's transcribe. This call may be slow on CPU.
        # For long files, consider setting `word_timestamps=True` if available for your version.
        result = self.model.transcribe(audio_path, language=language, task="transcribe")

        transcript_text = result.get("text", "").strip()
        segments = result.get("segments", [])

        return {
            "file_name": Path(audio_path).name,
            "transcript": transcript_text,
            "language": result.get("language", "unknown"),
            "segments": segments,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }

    def summarize_with_openai(self, transcript: str) -> Dict[str, Any]:
        """
        Use OpenAI ChatCompletion to generate structured summary.
        If OpenAI key is not set, this function should not be called.
        """
        prompt = f"""You are an assistant that analyzes a meeting transcript and returns a JSON object with keys:
- executive_summary: a concise 2-3 sentence overview
- key_decisions: list of important decisions (strings)
- action_items: list of action items (strings). If an owner is mentioned, include as 'Task - Owner: Name'
- discussion_topics: list of main topics
- next_steps: short list or sentence of next steps

Transcript:
{transcript}

Return strictly valid JSON with these keys.
"""
        # Use ChatCompletion (adjust model choice as needed)
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",  # change if not available in your account
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=600
        )
        content = response["choices"][0]["message"]["content"].strip()

        # Extract JSON substring if the model includes extraneous text
        m = re.search(r"(\{.*\})", content, re.S)
        json_text = m.group(1) if m else content
        try:
            parsed = json.loads(json_text)
        except Exception:
            # fallback: minimal parsed structure
            parsed = {
                "executive_summary": content[:800],
                "key_decisions": [],
                "action_items": [],
                "discussion_topics": [],
                "next_steps": ""
            }
        return parsed

    def fallback_summary(self, transcript: str) -> Dict[str, Any]:
        """
        Deterministic fallback summary without LLM:
        - executive_summary: first 3 sentences or first 80 words
        - key_decisions: []
        - action_items: extract lines starting with 'action' or 'todo' heuristically
        - discussion_topics: top frequent nouns/phrases (very simple)
        """
        # executive summary: first 3 sentences or first 80 words
        sentences = re.split(r'(?<=[.!?])\s+', transcript.strip())
        exec_sum = " ".join(sentences[:3]).strip()
        if not exec_sum:
            words = transcript.split()
            exec_sum = " ".join(words[:80]) + ("..." if len(words) > 80 else "")

        # Simple action item detection: look for lines that contain verbs like 'will', 'must', 'should', 'action'
        action_items = []
        for line in re.split(r'[\r\n]+', transcript):
            l = line.strip()
            if not l:
                continue
            if re.search(r'\b(action|todo|will|should|must|please|assign)\b', l, re.I):
                action_items.append(l.strip())

        # Key decisions heuristic: lines containing 'decide', 'agreed', 'decided', 'conclude'
        key_decisions = []
        for line in re.split(r'[\r\n]+', transcript):
            l = line.strip()
            if not l:
                continue
            if re.search(r'\b(decid|agree|conclud|approved)\b', l, re.I):
                key_decisions.append(l.strip())

        # Discussion topics: top frequent words excluding stopwords (very simple)
        words = re.findall(r'\w+', transcript.lower())
        stopwords = set([
            "the","and","is","in","to","of","for","we","you","that","it","on","with","as","are","this","be","have","not","or","by"
        ])
        freq = {}
        for w in words:
            if w in stopwords or w.isdigit() or len(w) < 3:
                continue
            freq[w] = freq.get(w, 0) + 1
        top_topics = sorted(freq.items(), key=lambda x: -x[1])[:8]
        discussion_topics = [t for t, _ in top_topics]

        return {
            "executive_summary": exec_sum,
            "key_decisions": key_decisions,
            "action_items": action_items,
            "discussion_topics": discussion_topics,
            "next_steps": ""
        }

    def generate_summary(self, transcript: str) -> Dict[str, Any]:
        """
        Generate summary using LLM if available, otherwise fallback.
        """
        if self.llm_available:
            try:
                return self.summarize_with_openai(transcript)
            except Exception as e:
                # If LLM call fails for any reason, fall back to deterministic summary
                return self.fallback_summary(transcript)
        else:
            return self.fallback_summary(transcript)

    def process_meeting(self, audio_path: str, language: str = None) -> Dict[str, Any]:
        """
        Full pipeline: transcribe, summarize, package results and store in memory.
        Returns the result dict.
        """
        transcript_data = self.transcribe_audio(audio_path, language=language)
        transcript_text = transcript_data["transcript"]
        summary = self.generate_summary(transcript_text)
        result = {
            "meeting_info": {
                "file_name": transcript_data["file_name"],
                "timestamp": transcript_data["timestamp"],
                "language": transcript_data["language"]
            },
            "transcript": transcript_text,
            "segments": transcript_data["segments"],
            "summary": summary
        }
        # Store in results dictionary keyed by filename or unique id
        key = transcript_data["file_name"] + "_" + uuid.uuid4().hex[:8]
        self.results[key] = result
        return result

    def save_json(self, result: Dict[str, Any], output_path: str):
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

    def print_summary(self, result: Dict[str, Any]):
        info = result["meeting_info"]
        print("FILE:", info["file_name"])
        print("TIMESTAMP:", info["timestamp"])
        print("\nEXECUTIVE SUMMARY\n-----------------")
        print(result["summary"].get("executive_summary", ""))
        print("\nKEY DECISIONS\n-------------")
        for d in result["summary"].get("key_decisions", []):
            print("-", d)
        print("\nACTION ITEMS\n------------")
        for a in result["summary"].get("action_items", []):
            print("-", a)
        print("\nDISCUSSION TOPICS\n-----------------")
        for t in result["summary"].get("discussion_topics", []):
            print("-", t)
        print("\nNEXT STEPS\n----------")
        print(result["summary"].get("next_steps", ""))

# Cell 4: PDF generation and evaluation utilities
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import mm

def save_summary_to_pdf(result: Dict[str, Any], pdf_path: str = "meeting_summary.pdf"):
    summary = result["summary"]
    info = result["meeting_info"]

    # PDF layout settings
    c = canvas.Canvas(pdf_path, pagesize=A4)
    width, height = A4
    left_margin = 20 * mm
    right_margin = 20 * mm
    y = height - 20 * mm
    line_height = 10  # points

    def writeline(text, fontname="Helvetica", fontsize=11, space_after=4):
        nonlocal y
        c.setFont(fontname, fontsize)
        # Simple wrap for long lines
        max_width = width - left_margin - right_margin
        from reportlab.lib.utils import simpleSplit
        lines = simpleSplit(text, fontname, fontsize, max_width)
        for ln in lines:
            c.drawString(left_margin, y, ln)
            y -= line_height
        y -= space_after

    # Header
    writeline("Meeting Summary Report", fontname="Helvetica-Bold", fontsize=14, space_after=8)
    writeline(f"File: {info['file_name']}")
    writeline(f"Timestamp (UTC): {info['timestamp']}")
    writeline(f"Language: {info.get('language', 'unknown')}", space_after=8)

    # Executive summary
    writeline("Executive Summary", fontname="Helvetica-Bold", fontsize=12, space_after=4)
    writeline(summary.get("executive_summary", ""), space_after=8)

    # Key decisions
    if summary.get("key_decisions"):
        writeline("Key Decisions", fontname="Helvetica-Bold", fontsize=12, space_after=4)
        for i, d in enumerate(summary.get("key_decisions", []), 1):
            writeline(f"{i}. {d}")
        writeline("", space_after=6)

    # Action items
    if summary.get("action_items"):
        writeline("Action Items", fontname="Helvetica-Bold", fontsize=12, space_after=4)
        for i, a in enumerate(summary.get("action_items", []), 1):
            writeline(f"{i}. {a}")
        writeline("", space_after=6)

    # Discussion topics
    if summary.get("discussion_topics"):
        writeline("Discussion Topics", fontname="Helvetica-Bold", fontsize=12, space_after=4)
        writeline(", ".join(summary.get("discussion_topics", [])), space_after=8)

    # Next steps
    if summary.get("next_steps"):
        writeline("Next Steps", fontname="Helvetica-Bold", fontsize=12, space_after=4)
        writeline(summary.get("next_steps", ""), space_after=8)

    c.showPage()
    c.save()
    print(f"Saved PDF to: {pdf_path}")

def compute_accuracy_metrics(result: Dict[str, Any], reference_transcript: str) -> Dict[str, float]:
    """
    Compute WER and approximate accuracy (%) from a reference transcript.
    Returns a dictionary with WER and Accuracy.
    """
    predicted = result.get("transcript", "")
    wer_score = wer(reference_transcript, predicted)
    accuracy_pct = max(0.0, (1.0 - wer_score)) * 100.0
    metrics = {"WER": wer_score, "AccuracyPercent": accuracy_pct}
    print("WER:", wer_score)
    print("Approximate accuracy (%):", f"{accuracy_pct:.2f}")
    return metrics

# Cell 5: Upload audio in Colab, process it, save JSON and PDF, optional evaluation
from google.colab import files
import requests
import os

# 1) Upload an audio file from local machine to Colab
print("Please select one audio file to upload (wav, mp3, m4a, etc.).")
uploaded = files.upload()  # this opens file browser in Colab

# Take the first uploaded file
audio_filename = next(iter(uploaded))
audio_path = os.path.join("/content", audio_filename)
print("Uploaded file:", audio_path)

# 2) Configure model size and whether to use GPU
WHISPER_MODEL = "base"   # options: tiny, base, small, medium, large
USE_GPU = False          # set True if GPU is enabled in Colab and you want faster transcription

# 3) Instantiate summarizer and run
summarizer = MeetingSummarizer(whisper_model=WHISPER_MODEL, use_gpu=USE_GPU)
result = summarizer.process_meeting(audio_path)

# 4) Print summary to console
summarizer.print_summary(result)

# 5) Save JSON and PDF
output_json = "meeting_summary.json"
output_pdf = "meeting_summary.pdf"
summarizer.save_json(result, output_json)
save_summary_to_pdf(result, output_pdf)

# 6) Optional: compute accuracy metrics if you have a ground-truth transcript
# Provide ground_truth_text as a string; if you do not have one, skip this step.
# Example:
ground_truth_text = ""  # Replace with actual ground-truth transcript if available
if ground_truth_text:
    metrics = compute_accuracy_metrics(result, ground_truth_text)
    # Save metrics into JSON
    result["metrics"] = metrics
    summarizer.save_json(result, "meeting_summary_with_metrics.json")

# 7) Download files to your machine (uncomment to automatically download)
# files.download(output_json)
files.download(output_pdf)